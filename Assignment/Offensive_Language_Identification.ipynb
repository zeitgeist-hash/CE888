{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Offensive Language Identification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "299f10519bce491194830a73f3817e67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_59d48115f1744dabab257fd0e23aa3cd",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_28c28fcad82e41dfa511d2b3f95570bd",
              "IPY_MODEL_6937e43b36af44419e39d6229912de9b"
            ]
          }
        },
        "59d48115f1744dabab257fd0e23aa3cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "28c28fcad82e41dfa511d2b3f95570bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_fff601f98bdd4f3ba00be92e7cb23cd2",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 479,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 479,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_972e5d6e2f6645cfbc349640be9f6c62"
          }
        },
        "6937e43b36af44419e39d6229912de9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f0606f4162404250a1da8ddd2406a898",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 479/479 [00:02&lt;00:00, 210B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8f993724ec36474e837dbae1c0beb617"
          }
        },
        "fff601f98bdd4f3ba00be92e7cb23cd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "972e5d6e2f6645cfbc349640be9f6c62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f0606f4162404250a1da8ddd2406a898": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8f993724ec36474e837dbae1c0beb617": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6c36dce253ca48afb70184288afa0281": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_aa2a3290b73d444e93434e84bc2e69f6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_640465723ed24f13b2f306f58dcde62a",
              "IPY_MODEL_2318fe69f5014736b0b9393ecbbc422b"
            ]
          }
        },
        "aa2a3290b73d444e93434e84bc2e69f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "640465723ed24f13b2f306f58dcde62a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_99a0815dace74a6180dd366f66645ac8",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e5553772bb05471f8de7f45f370064b1"
          }
        },
        "2318fe69f5014736b0b9393ecbbc422b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8b3d86e3849f4ce3bdde8721158d1209",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 490kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_36b8bd39e3404e37a479340bf9c98e69"
          }
        },
        "99a0815dace74a6180dd366f66645ac8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e5553772bb05471f8de7f45f370064b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8b3d86e3849f4ce3bdde8721158d1209": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "36b8bd39e3404e37a479340bf9c98e69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dcd1fa7b21494737baa77fcf84c6f5cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_cae14cb5b736459e85ee23e9f564befb",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_21ad73937c4049348afbf96b2a261b96",
              "IPY_MODEL_e2e8a00d9ff8459aaa154bc17062d18b"
            ]
          }
        },
        "cae14cb5b736459e85ee23e9f564befb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "21ad73937c4049348afbf96b2a261b96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_364786a18d034b35b84249c36a256664",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 112,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 112,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6d51d2b702a3402883492785115f40fa"
          }
        },
        "e2e8a00d9ff8459aaa154bc17062d18b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8d89f0fa722942699433f17224685e30",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 112/112 [00:00&lt;00:00, 206B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c99886ac088045f1b9074ad1ac0c3124"
          }
        },
        "364786a18d034b35b84249c36a256664": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6d51d2b702a3402883492785115f40fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8d89f0fa722942699433f17224685e30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c99886ac088045f1b9074ad1ac0c3124": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "16dd4ade86724cdb90c8285af00784b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_935acdb0d3744295b690752d4c433a05",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fbc6fef8b55c43eabccd0a559d8fd289",
              "IPY_MODEL_5dcac09299cd4ef09a4c389efa440931"
            ]
          }
        },
        "935acdb0d3744295b690752d4c433a05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fbc6fef8b55c43eabccd0a559d8fd289": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1b13874e76fd4c4c9ab6a51f309b76a6",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 62,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 62,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7a3aa958ebee4f569910fb84973c5ebf"
          }
        },
        "5dcac09299cd4ef09a4c389efa440931": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_44fe234c95ef4e689d9f414f58904511",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 62.0/62.0 [00:00&lt;00:00, 761B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ac5083624c364a3f8d79d91fe502503a"
          }
        },
        "1b13874e76fd4c4c9ab6a51f309b76a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7a3aa958ebee4f569910fb84973c5ebf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "44fe234c95ef4e689d9f414f58904511": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ac5083624c364a3f8d79d91fe502503a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fcd1f8c02ec5444dadca45ac160a55db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c698da28e21b4e2b96197ca05bab94dd",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_94edb93d9880408d8e5314163579cb44",
              "IPY_MODEL_646722514ed04b5581494836dcab595a"
            ]
          }
        },
        "c698da28e21b4e2b96197ca05bab94dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "94edb93d9880408d8e5314163579cb44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d27f65e5803d4c7aacd81408fa62d19b",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440474579,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440474579,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_652126ac315042128de2d6962d7844eb"
          }
        },
        "646722514ed04b5581494836dcab595a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2b1f09b9ab614b2299037b6f2aa78387",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [00:16&lt;00:00, 26.4MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_756cb2578fa7479eb0516da04e9e2001"
          }
        },
        "d27f65e5803d4c7aacd81408fa62d19b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "652126ac315042128de2d6962d7844eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2b1f09b9ab614b2299037b6f2aa78387": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "756cb2578fa7479eb0516da04e9e2001": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhp6GG1qZMks"
      },
      "source": [
        "### Offensive Language Identification\n",
        "\n",
        "ERNIE 2.0 is a continual pre-training framework proposed by Baidu in 2019, which builds and learns incrementally pre-training tasks through constant multi-task learning.\n",
        "This project will convert ERNIE to huggingface's format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fB-trh1KZTtn"
      },
      "source": [
        "### Set up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRBY6XA4YtXQ",
        "outputId": "e0470705-710b-44bd-a7b7-7e762821a2a8"
      },
      "source": [
        "pip install paddlepaddle"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting paddlepaddle\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/e2/1a706d6ce62aae1107c406f1bdd23cf7a6810bc851e04ec9a393deae097b/paddlepaddle-2.0.2-cp37-cp37m-manylinux1_x86_64.whl (168.5MB)\n",
            "\u001b[K     |████████████████████████████████| 168.5MB 27kB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from paddlepaddle) (7.1.2)\n",
            "Requirement already satisfied: gast>=0.3.3; platform_system != \"Windows\" in /usr/local/lib/python3.7/dist-packages (from paddlepaddle) (0.3.3)\n",
            "Requirement already satisfied: protobuf>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from paddlepaddle) (3.12.4)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from paddlepaddle) (0.8.1)\n",
            "Requirement already satisfied: numpy>=1.13; python_version >= \"3.5\" and platform_system != \"Windows\" in /usr/local/lib/python3.7/dist-packages (from paddlepaddle) (1.19.5)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from paddlepaddle) (2.23.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from paddlepaddle) (4.4.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from paddlepaddle) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.1.0->paddlepaddle) (56.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->paddlepaddle) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->paddlepaddle) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->paddlepaddle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->paddlepaddle) (3.0.4)\n",
            "Installing collected packages: paddlepaddle\n",
            "Successfully installed paddlepaddle-2.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1FpdIeNW5Dr",
        "outputId": "391c75be-d422-4a7b-bb8e-55567acaf95c"
      },
      "source": [
        "pip install paddle-ernie"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting paddle-ernie\n",
            "  Downloading https://files.pythonhosted.org/packages/56/c0/ebea990ff37baedc6f929241278c24ebb40f538f48c52b567d8afbd1dfb0/paddle-ernie-0.1.0.dev1.tar.gz\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from paddle-ernie) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from paddle-ernie) (4.41.1)\n",
            "Collecting pathlib2\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/45/9c82d3666af4ef9f221cbb954e1d77ddbb513faf552aea6df5f37f1a4859/pathlib2-2.3.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->paddle-ernie) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->paddle-ernie) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->paddle-ernie) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->paddle-ernie) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pathlib2->paddle-ernie) (1.15.0)\n",
            "Building wheels for collected packages: paddle-ernie\n",
            "  Building wheel for paddle-ernie (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for paddle-ernie: filename=paddle_ernie-0.1.0.dev1-cp37-none-any.whl size=20016 sha256=d1a5c481e188f8bab149866781500ac5b1e5b929bf01376828ad767cc999c0ee\n",
            "  Stored in directory: /root/.cache/pip/wheels/ef/d8/52/d0fc0e19bce14963259ada61138e6c520805bd688e29652796\n",
            "Successfully built paddle-ernie\n",
            "Installing collected packages: pathlib2, paddle-ernie\n",
            "Successfully installed paddle-ernie-0.1.0.dev1 pathlib2-2.3.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_J3KEz-juCY-"
      },
      "source": [
        "pip install --upgrade paddlenlp>=2.0.0rc -i https://pypi.org/simple"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VzU-GBo9jTE",
        "outputId": "61d303ea-98e1-4fe3-e163-9ab1a1bb3682"
      },
      "source": [
        "pip install sentencepiece"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\r\u001b[K     |▎                               | 10kB 16.3MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 21.9MB/s eta 0:00:01\r\u001b[K     |▉                               | 30kB 17.1MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 14.9MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51kB 10.2MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61kB 10.7MB/s eta 0:00:01\r\u001b[K     |██                              | 71kB 9.3MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81kB 10.1MB/s eta 0:00:01\r\u001b[K     |██▌                             | 92kB 9.9MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102kB 9.4MB/s eta 0:00:01\r\u001b[K     |███                             | 112kB 9.4MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122kB 9.4MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133kB 9.4MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143kB 9.4MB/s eta 0:00:01\r\u001b[K     |████                            | 153kB 9.4MB/s eta 0:00:01\r\u001b[K     |████▍                           | 163kB 9.4MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174kB 9.4MB/s eta 0:00:01\r\u001b[K     |█████                           | 184kB 9.4MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194kB 9.4MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 204kB 9.4MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215kB 9.4MB/s eta 0:00:01\r\u001b[K     |██████                          | 225kB 9.4MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 235kB 9.4MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245kB 9.4MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 256kB 9.4MB/s eta 0:00:01\r\u001b[K     |███████                         | 266kB 9.4MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 276kB 9.4MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286kB 9.4MB/s eta 0:00:01\r\u001b[K     |████████                        | 296kB 9.4MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 307kB 9.4MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317kB 9.4MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 327kB 9.4MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337kB 9.4MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 348kB 9.4MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358kB 9.4MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 368kB 9.4MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378kB 9.4MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 389kB 9.4MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 399kB 9.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 409kB 9.4MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 419kB 9.4MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430kB 9.4MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 440kB 9.4MB/s eta 0:00:01\r\u001b[K     |████████████                    | 450kB 9.4MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 460kB 9.4MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 471kB 9.4MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 481kB 9.4MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491kB 9.4MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 501kB 9.4MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 512kB 9.4MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 522kB 9.4MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 532kB 9.4MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 542kB 9.4MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 552kB 9.4MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 563kB 9.4MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 573kB 9.4MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 583kB 9.4MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 593kB 9.4MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604kB 9.4MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 614kB 9.4MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 624kB 9.4MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 634kB 9.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 645kB 9.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 655kB 9.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 665kB 9.4MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 675kB 9.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 686kB 9.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 696kB 9.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 706kB 9.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716kB 9.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 727kB 9.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 737kB 9.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 747kB 9.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 757kB 9.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 768kB 9.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 778kB 9.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 788kB 9.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 798kB 9.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 808kB 9.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 819kB 9.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 829kB 9.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 839kB 9.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 849kB 9.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 860kB 9.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 870kB 9.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 880kB 9.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 890kB 9.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 901kB 9.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 911kB 9.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 921kB 9.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 931kB 9.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 942kB 9.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 952kB 9.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 962kB 9.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 972kB 9.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 983kB 9.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 993kB 9.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0MB 9.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0MB 9.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0MB 9.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0MB 9.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.0MB 9.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1MB 9.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.1MB 9.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1MB 9.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.1MB 9.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1MB 9.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1MB 9.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1MB 9.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1MB 9.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.1MB 9.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.1MB 9.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2MB 9.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2MB 9.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2MB 9.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2MB 9.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2MB 9.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2MB 9.4MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-o93hCPsE-M",
        "outputId": "954cb70c-f9db-4f8c-8623-cd379808be2c"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 35.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 48.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuwlFMk5QUcr",
        "outputId": "851df7a1-c2a1-4d7e-d763-f9fc50ca339d"
      },
      "source": [
        "import torch\n",
        "import paddle\n",
        "import paddlenlp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from textwrap import wrap\n",
        "from torch import nn, optim\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import f1_score\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "from ernie.modeling_ernie import ErnieModel\n",
        "from ernie.tokenizing_ernie import ErnieTokenizer\n",
        "from ernie.modeling_ernie import ErnieModelForSequenceClassification"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/packaging/version.py:130: DeprecationWarning: Creating a LegacyVersion has been deprecated and will be removed in the next major release\n",
            "  DeprecationWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRVOwaCCpcY7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459,
          "referenced_widgets": [
            "299f10519bce491194830a73f3817e67",
            "59d48115f1744dabab257fd0e23aa3cd",
            "28c28fcad82e41dfa511d2b3f95570bd",
            "6937e43b36af44419e39d6229912de9b",
            "fff601f98bdd4f3ba00be92e7cb23cd2",
            "972e5d6e2f6645cfbc349640be9f6c62",
            "f0606f4162404250a1da8ddd2406a898",
            "8f993724ec36474e837dbae1c0beb617",
            "6c36dce253ca48afb70184288afa0281",
            "aa2a3290b73d444e93434e84bc2e69f6",
            "640465723ed24f13b2f306f58dcde62a",
            "2318fe69f5014736b0b9393ecbbc422b",
            "99a0815dace74a6180dd366f66645ac8",
            "e5553772bb05471f8de7f45f370064b1",
            "8b3d86e3849f4ce3bdde8721158d1209",
            "36b8bd39e3404e37a479340bf9c98e69",
            "dcd1fa7b21494737baa77fcf84c6f5cb",
            "cae14cb5b736459e85ee23e9f564befb",
            "21ad73937c4049348afbf96b2a261b96",
            "e2e8a00d9ff8459aaa154bc17062d18b",
            "364786a18d034b35b84249c36a256664",
            "6d51d2b702a3402883492785115f40fa",
            "8d89f0fa722942699433f17224685e30",
            "c99886ac088045f1b9074ad1ac0c3124",
            "16dd4ade86724cdb90c8285af00784b3",
            "935acdb0d3744295b690752d4c433a05",
            "fbc6fef8b55c43eabccd0a559d8fd289",
            "5dcac09299cd4ef09a4c389efa440931",
            "1b13874e76fd4c4c9ab6a51f309b76a6",
            "7a3aa958ebee4f569910fb84973c5ebf",
            "44fe234c95ef4e689d9f414f58904511",
            "ac5083624c364a3f8d79d91fe502503a",
            "fcd1f8c02ec5444dadca45ac160a55db",
            "c698da28e21b4e2b96197ca05bab94dd",
            "94edb93d9880408d8e5314163579cb44",
            "646722514ed04b5581494836dcab595a",
            "d27f65e5803d4c7aacd81408fa62d19b",
            "652126ac315042128de2d6962d7844eb",
            "2b1f09b9ab614b2299037b6f2aa78387",
            "756cb2578fa7479eb0516da04e9e2001"
          ]
        },
        "outputId": "7ccbffd5-09ab-45d6-c2c5-d1efb9df49e6"
      },
      "source": [
        "# Load pre-trained ERNIE 2.0 model and ERNIE tokenizer.\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"nghuyong/ernie-2.0-en\")\n",
        "\n",
        "model = AutoModel.from_pretrained(\"nghuyong/ernie-2.0-en\", return_dict=False)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-28 03:37:48,338 - INFO - Lock 140418959259664 acquired on /root/.cache/huggingface/transformers/22d34926689d46e875370d2df90e2b40c9e2f6a754003f3e4f520890ed4b2941.348bf454a4de5dea8471d837bcfb348dc42bca3b7b27f2e07fc244be6ad85f1f.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "299f10519bce491194830a73f3817e67",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=479.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "2021-04-28 03:37:48,600 - INFO - Lock 140418959259664 released on /root/.cache/huggingface/transformers/22d34926689d46e875370d2df90e2b40c9e2f6a754003f3e4f520890ed4b2941.348bf454a4de5dea8471d837bcfb348dc42bca3b7b27f2e07fc244be6ad85f1f.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-04-28 03:37:48,812 - INFO - Lock 140418933378640 acquired on /root/.cache/huggingface/transformers/1ab10693e539d85024e9dfd259048b22e64a07e891b6d867a080b79b40db1b62.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c36dce253ca48afb70184288afa0281",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "2021-04-28 03:37:49,287 - INFO - Lock 140418933378640 released on /root/.cache/huggingface/transformers/1ab10693e539d85024e9dfd259048b22e64a07e891b6d867a080b79b40db1b62.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-04-28 03:37:50,067 - INFO - Lock 140421761209104 acquired on /root/.cache/huggingface/transformers/33f8794015698a2d68c128ff2ccc8bf54d1faebb956adf949a3be00ad6496115.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dcd1fa7b21494737baa77fcf84c6f5cb",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=112.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "2021-04-28 03:37:50,320 - INFO - Lock 140421761209104 released on /root/.cache/huggingface/transformers/33f8794015698a2d68c128ff2ccc8bf54d1faebb956adf949a3be00ad6496115.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-04-28 03:37:50,529 - INFO - Lock 140418933426576 acquired on /root/.cache/huggingface/transformers/adffd3dc92cda16f89403840540d8a26fa86a68be8074087fd0b808d71bc9a7a.1788df22ba1a6817edb607a56efa931ee13ebad3b3500e58029a8f4e6d799a29.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "16dd4ade86724cdb90c8285af00784b3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=62.0, style=ProgressStyle(description_w…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "2021-04-28 03:37:50,787 - INFO - Lock 140418933426576 released on /root/.cache/huggingface/transformers/adffd3dc92cda16f89403840540d8a26fa86a68be8074087fd0b808d71bc9a7a.1788df22ba1a6817edb607a56efa931ee13ebad3b3500e58029a8f4e6d799a29.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-04-28 03:37:51,275 - INFO - Lock 140418933378384 acquired on /root/.cache/huggingface/transformers/e4590b8215d335899d48d7b5d9a080f33663a611731a758597836bb837c70a3c.67e876ab5ebe858ae9e766f73e37366bab7f513423a551f2b220bbbabe79a026.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fcd1f8c02ec5444dadca45ac160a55db",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440474579.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "2021-04-28 03:38:07,811 - INFO - Lock 140418933378384 released on /root/.cache/huggingface/transformers/e4590b8215d335899d48d7b5d9a080f33663a611731a758597836bb837c70a3c.67e876ab5ebe858ae9e766f73e37366bab7f513423a551f2b220bbbabe79a026.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zRZrZLIhXSx"
      },
      "source": [
        "### Data Exploration\n",
        "\n",
        "We will load the irony dataset from TweetEval to evaluate ERNIE 2.0 model. The irony dataset has already been split into training set, tuning set and test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MZZ9VZsUacJ"
      },
      "source": [
        "df_train_text = pd.read_csv(\"https://raw.githubusercontent.com/zeitgeist-hash/tweeteval/main/datasets/offensive/train_text.txt\", delimiter=\"\\t\", names=[\"Tweet\"])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsAiVPzHUa-4"
      },
      "source": [
        "df_train_label = pd.read_csv(\"https://raw.githubusercontent.com/zeitgeist-hash/tweeteval/main/datasets/offensive/train_labels.txt\", delimiter=\"\\t\", names=[\"Label\"])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "cJ1cjplbUbBc",
        "outputId": "6841aac1-3d6a-4003-f0e9-2c35b4eb37b4"
      },
      "source": [
        "df_train = df_train_text.join(df_train_label)\n",
        "df_train.to_csv(\"train.csv\", encoding='utf-8', index=False)\n",
        "df_train"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@user Bono... who cares. Soon people will unde...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@user Eight years the republicans denied obama...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@user Get him some line help. He is gonna be j...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@user @user She is great. Hi Fiona!</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@user She has become a parody unto herself? Sh...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11911</th>\n",
              "      <td>@user I wonder if they are sex traffic victims?</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11912</th>\n",
              "      <td>@user Do we dare say he is... better than Nyjer?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11913</th>\n",
              "      <td>@user No idea who he is. Sorry</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11914</th>\n",
              "      <td>#Professor Who Shot Self Over Trump Says Gun C...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11915</th>\n",
              "      <td>@user @user @user Here your proof!  Our Africa...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11916 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   Tweet  Label\n",
              "0      @user Bono... who cares. Soon people will unde...      0\n",
              "1      @user Eight years the republicans denied obama...      1\n",
              "2      @user Get him some line help. He is gonna be j...      0\n",
              "3                   @user @user She is great. Hi Fiona!       0\n",
              "4      @user She has become a parody unto herself? Sh...      1\n",
              "...                                                  ...    ...\n",
              "11911   @user I wonder if they are sex traffic victims?       1\n",
              "11912  @user Do we dare say he is... better than Nyjer?       0\n",
              "11913                    @user No idea who he is. Sorry       0\n",
              "11914  #Professor Who Shot Self Over Trump Says Gun C...      0\n",
              "11915  @user @user @user Here your proof!  Our Africa...      1\n",
              "\n",
              "[11916 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z956FCSysMiZ"
      },
      "source": [
        "df_val_text = pd.read_csv(\"https://raw.githubusercontent.com/zeitgeist-hash/tweeteval/main/datasets/offensive/val_text.txt\", delimiter=\"\\t\", names=[\"Tweet\"])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXG2ex84sMu5"
      },
      "source": [
        "df_val_label = pd.read_csv(\"https://raw.githubusercontent.com/zeitgeist-hash/tweeteval/main/datasets/offensive/val_labels.txt\", delimiter=\"\\t\", names=[\"Label\"])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "Vk-UqGjRsNRk",
        "outputId": "30e31cdd-87c1-4b3c-b22e-5ce425e60f49"
      },
      "source": [
        "df_val = df_val_text.join(df_val_label)\n",
        "df_val.to_csv(\"val.csv\", encoding='utf-8', index=False)\n",
        "df_val"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@user @user WiiU is not even a real console.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@user @user @user If he is from AZ I would put...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@user I thought Canada had strict gun control....</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@user @user @user @user @user @user @user @use...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1 Minute of Truth: Gun Control via @user</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1319</th>\n",
              "      <td>@user @user Whose twitter interest start with ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1320</th>\n",
              "      <td>@user @user How did the press\"\" get the letter...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1321</th>\n",
              "      <td>@user @user @user @user @user @user Sorry abou...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1322</th>\n",
              "      <td>@user Fuck Alan I’m sorry</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1323</th>\n",
              "      <td>#Tories #Labour #GE2017 #Conservatives Conserv...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1324 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  Tweet  Label\n",
              "0         @user @user WiiU is not even a real console.       0\n",
              "1     @user @user @user If he is from AZ I would put...      1\n",
              "2     @user I thought Canada had strict gun control....      0\n",
              "3     @user @user @user @user @user @user @user @use...      0\n",
              "4             1 Minute of Truth: Gun Control via @user       0\n",
              "...                                                 ...    ...\n",
              "1319  @user @user Whose twitter interest start with ...      0\n",
              "1320  @user @user How did the press\"\" get the letter...      0\n",
              "1321  @user @user @user @user @user @user Sorry abou...      0\n",
              "1322                         @user Fuck Alan I’m sorry       1\n",
              "1323  #Tories #Labour #GE2017 #Conservatives Conserv...      0\n",
              "\n",
              "[1324 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huvaiBrVcHUQ"
      },
      "source": [
        "df_test_text = pd.read_csv(\"https://raw.githubusercontent.com/zeitgeist-hash/tweeteval/main/datasets/offensive/test_text.txt\", delimiter=\"\\t\", names=[\"Tweet\"])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcGxSie2VKCZ"
      },
      "source": [
        "df_test_label = pd.read_csv(\"https://raw.githubusercontent.com/zeitgeist-hash/tweeteval/main/datasets/offensive/test_labels.txt\", delimiter=\"\\t\", names=[\"Label\"])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "lXNaiomvVcpe",
        "outputId": "2ac8cf18-6a34-4fab-b79c-6778c8e9b129"
      },
      "source": [
        "df_test = df_test_text.join(df_test_label)\n",
        "df_test.to_csv(\"test.csv\", encoding='utf-8', index=False)\n",
        "df_test"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>#ibelieveblaseyford is liar she is fat ugly li...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@user @user @user I got in a pretty deep debat...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>...if you want more shootings and more death, ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Angels now have 6 runs. Five of them have come...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>#Travel #Movies and Unix #Fortune combined  Vi...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>855</th>\n",
              "      <td>#CNN irrationally argues 4 legalising #abortio...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>856</th>\n",
              "      <td>@user @user @user @user @user @user @user @use...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>857</th>\n",
              "      <td>#Conservatives don’t care what you post..it’s ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>858</th>\n",
              "      <td>#antifa #Resist.. Trump is trying to bring wor...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>859</th>\n",
              "      <td>#Maine you need to face facts @user doesn’t re...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>860 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 Tweet  Label\n",
              "0    #ibelieveblaseyford is liar she is fat ugly li...      1\n",
              "1    @user @user @user I got in a pretty deep debat...      0\n",
              "2    ...if you want more shootings and more death, ...      0\n",
              "3    Angels now have 6 runs. Five of them have come...      0\n",
              "4    #Travel #Movies and Unix #Fortune combined  Vi...      0\n",
              "..                                                 ...    ...\n",
              "855  #CNN irrationally argues 4 legalising #abortio...      0\n",
              "856  @user @user @user @user @user @user @user @use...      0\n",
              "857  #Conservatives don’t care what you post..it’s ...      1\n",
              "858  #antifa #Resist.. Trump is trying to bring wor...      0\n",
              "859  #Maine you need to face facts @user doesn’t re...      0\n",
              "\n",
              "[860 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPo91oLyIH97",
        "outputId": "ea68fdc4-5791-429b-9e98-38ad00d2d76a"
      },
      "source": [
        "df_train.shape, df_val.shape, df_test.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((11916, 2), (1324, 2), (860, 2))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "14qKPO-FTN6F",
        "outputId": "54bf2c8e-f309-4b11-8cdf-ab0297f9dd3e"
      },
      "source": [
        "# Training set\n",
        "\n",
        "sns.countplot(df_train['Label'])\n",
        "plt.xlabel('label class')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'label class')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "gmaEeARGTqL7",
        "outputId": "08c4e29d-62f2-4766-feeb-d4a878f8466d"
      },
      "source": [
        "# Validation set\n",
        "\n",
        "sns.countplot(df_val['Label'])\n",
        "plt.xlabel('label class')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'label class')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "_j_WQPPXTr8R",
        "outputId": "98f1f2cb-71e4-4b05-9cdf-759bec0da8de"
      },
      "source": [
        "# Test set\n",
        "\n",
        "sns.countplot(df_test['Label'])\n",
        "plt.xlabel('label class')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'label class')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAQvUlEQVR4nO3de7CdVX3G8e8DAaxXQI4REzRUUy3VgnDGIraOymiBtoY6iHdSmpm0U+q10xY7ndo6taPTWgpemEnlEizVUiySWkal8VZbtR40AhIZU0aaZIAcFfHCoEV//eOsLDbJCTkR3rMPnO9nZs9e71rrfffvZM6cJ++797t2qgpJkgD2G3cBkqSFw1CQJHWGgiSpMxQkSZ2hIEnqloy7gPvjsMMOqxUrVoy7DEl6ULnmmmu+WVUTs409qENhxYoVTE1NjbsMSXpQSXLznsa8fCRJ6gwFSVJnKEiSOkNBktQZCpKkzlCQJHWGgiSpMxQkSZ2hIEnqHtR3ND8QjvvDS8Zdghaga/76jHGXII2FZwqSpM5QkCR1g4ZCkoOTXJ7ka0k2J3l2kkOTXJ3k6+35kDY3Sc5LsiXJtUmOHbI2SdLuhj5TOBf4aFU9DTga2AycDWysqpXAxrYNcDKwsj3WAucPXJskaReDhUKSxwDPBS4AqKofVdV3gFXA+jZtPXBqa68CLqkZnwcOTnL4UPVJknY35JnCkcA0cFGSLyd5X5JHAEur6pY251ZgaWsvA7aO7L+t9d1LkrVJppJMTU9PD1i+JC0+Q4bCEuBY4PyqeibwA+65VARAVRVQ+3LQqlpXVZNVNTkxMesXB0mSfkpDhsI2YFtVfaFtX85MSNy287JQe97RxrcDR4zsv7z1SZLmyWChUFW3AluTPLV1nQjcAGwAVre+1cCVrb0BOKN9Cul44I6Ry0ySpHkw9B3NrwUuTXIgcBNwJjNBdFmSNcDNwOlt7lXAKcAW4M42V5I0jwYNharaBEzOMnTiLHMLOGvIeiRJ9807miVJnaEgSeoMBUlSZyhIkjpDQZLUGQqSpM5QkCR1hoIkqTMUJEmdoSBJ6gwFSVJnKEiSOkNBktQZCpKkzlCQJHWGgiSpMxQkSZ2hIEnqDAVJUmcoSJI6Q0GS1BkKkqTOUJAkdYaCJKkzFCRJ3aChkOQbSa5LsinJVOs7NMnVSb7eng9p/UlyXpItSa5NcuyQtUmSdjcfZwrPr6pjqmqybZ8NbKyqlcDGtg1wMrCyPdYC589DbZKkEeO4fLQKWN/a64FTR/ovqRmfBw5OcvgY6pOkRWvoUCjg40muSbK29S2tqlta+1ZgaWsvA7aO7Lut9d1LkrVJppJMTU9PD1W3JC1KSwY+/i9X1fYkjwOuTvK10cGqqiS1LwesqnXAOoDJycl92leSdN8GPVOoqu3teQdwBfAs4Ladl4Xa8442fTtwxMjuy1ufJGmeDBYKSR6R5FE728CLgOuBDcDqNm01cGVrbwDOaJ9COh64Y+QykyRpHgx5+WgpcEWSna/zj1X10SRfBC5Lsga4GTi9zb8KOAXYAtwJnDlgbZKkWQwWClV1E3D0LP3fAk6cpb+As4aqR5K0d97RLEnqDAVJUmcoSJI6Q0GS1BkKkqTOUJAkdYaCJKkzFCRJnaEgSeoMBUlSZyhIkjpDQZLUGQqSpM5QkCR1hoIkqTMUJEmdoSBJ6gwFSVJnKEiSOkNBktQZCpKkzlCQJHWGgiSpMxQkSd3goZBk/yRfTvKRtn1kki8k2ZLkn5Ic2PoPattb2viKoWuTJN3bfJwpvB7YPLL9DuCcqnoKcDuwpvWvAW5v/ee0eZKkeTRoKCRZDvwa8L62HeAFwOVtynrg1NZe1bZp4ye2+ZKkeTL0mcLfAX8E/KRtPxb4TlXd3ba3ActaexmwFaCN39Hm30uStUmmkkxNT08PWbskLTqDhUKSXwd2VNU1D+Rxq2pdVU1W1eTExMQDeWhJWvSWDHjs5wAvTnIK8DDg0cC5wMFJlrSzgeXA9jZ/O3AEsC3JEuAxwLcGrE+StIvBzhSq6s1VtbyqVgAvBz5RVa8CPgmc1qatBq5s7Q1tmzb+iaqqoeqTJO1uHPcp/DHwpiRbmHnP4ILWfwHw2Nb/JuDsMdQmSYvakJePuqr6FPCp1r4JeNYsc+4CXjof9UiSZucdzZKkzlCQJHWGgiSpMxQkSZ2hIEnqDAVJUmcoSJI6Q0GS1M0pFJJsnEufJOnB7T7vaE7yMODhwGFJDgF2fr/Bo7lnyWtJ0kPE3pa5+B3gDcATgGu4JxS+C7x7wLokSWNwn6FQVecC5yZ5bVW9a55qkiSNyZwWxKuqdyU5AVgxuk9VXTJQXZKkMZhTKCR5P/BkYBPw49ZdgKEgSQ8hc106exI4yi+9kaSHtrnep3A98PghC5Ekjd9czxQOA25I8t/AD3d2VtWLB6lKkjQWcw2FPx+yCEnSwjDXTx99euhCJEnjN9dPH32PmU8bARwIHAD8oKoePVRhkqT5N9czhUftbCcJsAo4fqiiJEnjsc+rpNaMDwO/OkA9kqQxmuvlo5eMbO7HzH0Ldw1SkSRpbOb66aPfGGnfDXyDmUtIkqSHkLm+p3Dmvh64Lbv9GeCg9jqXV9VbkhwJfBB4LDMrr76mqn6U5CBmls04DvgW8LKq+sa+vq4k6ac31y/ZWZ7kiiQ72uNDSZbvZbcfAi+oqqOBY4CTkhwPvAM4p6qeAtwOrGnz1wC3t/5z2jxJ0jya6xvNFwEbmPlehScA/9r69qi9If39tnlAexTwAuDy1r8eOLW1V7Vt2viJ7ZNOkqR5MtdQmKiqi6rq7va4GJjY205J9k+yCdgBXA38D/Cdqrq7TdnGPd/gtgzYCtDG72DmEpMkaZ7MNRS+leTV7Y/8/klezcx1//tUVT+uqmOA5cCzgKfdj1oBSLI2yVSSqenp6ft7OEnSiLmGwm8DpwO3ArcApwG/NdcXqarvAJ8Eng0cnGTnG9zLge2tvR04AqCNP4ZZgqeq1lXVZFVNTkzs9WRFkrQP5hoKbwVWV9VEVT2OmZD4i/vaIclEkoNb+2eAFwKbmQmH09q01cCVrb2hbdPGP+H3N0jS/JrrfQq/WFW379yoqm8neeZe9jkcWJ9kf2bC57Kq+kiSG4APJvlL4MvABW3+BcD7k2wBvg28fF9+EEnS/TfXUNgvySE7gyHJoXvbt6quBXYLjqq6iZn3F3btvwt46RzrkSQNYK6h8E7gc0n+uW2/FHjbMCVJksZlrnc0X5Jkipl7DABeUlU3DFeWJGkc5nqmQAsBg0CSHsL2eelsSdJDl6EgSeoMBUlSZyhIkjpDQZLUGQqSpM5QkCR1hoIkqTMUJEmdoSBJ6gwFSVJnKEiSujkviCdpfv3vW58x7hK0AD3xz64b9PieKUiSOkNBktQZCpKkzlCQJHWGgiSpMxQkSZ2hIEnqDAVJUmcoSJK6wUIhyRFJPpnkhiRfTfL61n9okquTfL09H9L6k+S8JFuSXJvk2KFqkyTNbsgzhbuBP6iqo4DjgbOSHAWcDWysqpXAxrYNcDKwsj3WAucPWJskaRaDhUJV3VJVX2rt7wGbgWXAKmB9m7YeOLW1VwGX1IzPAwcnOXyo+iRJu5uX9xSSrACeCXwBWFpVt7ShW4Glrb0M2Dqy27bWt+ux1iaZSjI1PT09WM2StBgNHgpJHgl8CHhDVX13dKyqCqh9OV5VrauqyaqanJiYeAArlSQNGgpJDmAmEC6tqn9p3bftvCzUnne0/u3AESO7L299kqR5MuSnjwJcAGyuqr8dGdoArG7t1cCVI/1ntE8hHQ/cMXKZSZI0D4b8kp3nAK8BrkuyqfX9CfB24LIka4CbgdPb2FXAKcAW4E7gzAFrkyTNYrBQqKrPAtnD8ImzzC/grKHqkSTtnXc0S5I6Q0GS1BkKkqTOUJAkdYaCJKkzFCRJnaEgSeoMBUlSZyhIkjpDQZLUGQqSpM5QkCR1hoIkqTMUJEmdoSBJ6gwFSVJnKEiSOkNBktQZCpKkzlCQJHWGgiSpMxQkSZ2hIEnqDAVJUjdYKCS5MMmOJNeP9B2a5OokX2/Ph7T+JDkvyZYk1yY5dqi6JEl7NuSZwsXASbv0nQ1srKqVwMa2DXAysLI91gLnD1iXJGkPBguFqvoM8O1dulcB61t7PXDqSP8lNePzwMFJDh+qNknS7Ob7PYWlVXVLa98KLG3tZcDWkXnbWt9ukqxNMpVkanp6erhKJWkRGtsbzVVVQP0U+62rqsmqmpyYmBigMklavOY7FG7beVmoPe9o/duBI0bmLW99kqR5NN+hsAFY3dqrgStH+s9on0I6Hrhj5DKTJGmeLBnqwEk+ADwPOCzJNuAtwNuBy5KsAW4GTm/TrwJOAbYAdwJnDlWXJGnPBguFqnrFHoZOnGVuAWcNVYskaW68o1mS1BkKkqTOUJAkdYaCJKkzFCRJnaEgSeoMBUlSZyhIkjpDQZLUGQqSpM5QkCR1hoIkqTMUJEmdoSBJ6gwFSVJnKEiSOkNBktQZCpKkzlCQJHWGgiSpMxQkSZ2hIEnqDAVJUmcoSJI6Q0GS1C2oUEhyUpIbk2xJcva465GkxWbBhEKS/YH3ACcDRwGvSHLUeKuSpMVlwYQC8CxgS1XdVFU/Aj4IrBpzTZK0qCwZdwEjlgFbR7a3Ab+066Qka4G1bfP7SW6ch9oWi8OAb467iIUgf7N63CXo3vzd3OkteSCO8qQ9DSykUJiTqloHrBt3HQ9FSaaqanLcdUi78ndz/iyky0fbgSNGtpe3PknSPFlIofBFYGWSI5McCLwc2DDmmiRpUVkwl4+q6u4kvw98DNgfuLCqvjrmshYbL8tpofJ3c56kqsZdgyRpgVhIl48kSWNmKEiSOkNBLi+iBSvJhUl2JLl+3LUsFobCIufyIlrgLgZOGncRi4mhIJcX0YJVVZ8Bvj3uOhYTQ0GzLS+ybEy1SBozQ0GS1BkKcnkRSZ2hIJcXkdQZCotcVd0N7FxeZDNwmcuLaKFI8gHgc8BTk2xLsmbcNT3UucyFJKnzTEGS1BkKkqTOUJAkdYaCJKkzFCRJnaGgRSfJ9/cyvmJfV+VMcnGS0+Y4d5+PL80XQ0GS1BkKWrSSPDLJxiRfSnJdktHVYZckuTTJ5iSXJ3l42+e4JJ9Ock2SjyU5fC+v8ZQk/57kK+11nrzL+Iok/9HGvpTkhNZ/eJLPJNmU5Pokv5Jk/3ZGcn2r940P+D+KFj1DQYvZXcBvVtWxwPOBdyZJG3sq8N6q+nngu8DvJTkAeBdwWlUdB1wIvG0vr3Ep8J6qOho4Abhll/EdwAtbDS8Dzmv9rwQ+VlXHAEcDm4BjgGVV9fSqegZw0U/7g0t7smTcBUhjFOCvkjwX+AkzS4YvbWNbq+o/W/sfgNcBHwWeDlzdsmN/dv8jf8/Bk0cx80f8CoCquqv1j047AHh3kmOAHwM/1/q/CFzYgujDVbUpyU3AzyZ5F/BvwMfvx88uzcozBS1mrwImgOPa/8hvAx7WxnZd/6WYCZGvVtUx7fGMqnrR/azhje11jwYmgQOhf7nMc5lZsfbiJGdU1e1t3qeA3wXedz9fW9qNoaDF7DHAjqr6vyTPB540MvbEJM9u7VcCnwVuBCZ29ic5IMkv7OngVfU9YFuSU9v8g3a+N7FLDbdU1U+A1zBz9kGSJwG3VdXfM/PH/9gkhwH7VdWHgD8Fjr0/P7w0G0NBi9mlwGSS64AzgK+NjN0InJVkM3AIcH77utLTgHck+Qoz1/lP2MtrvAZ4XZJrgf8CHr/L+HuB1e14TwN+0PqfB3wlyZeZea/hXGYub30qySZmLmm9ed9/ZOm+uUqqJKnzTEGS1BkKkqTOUJAkdYaCJKkzFCRJnaEgSeoMBUlS9/8k+0QeWWgaOwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jMwA9sPkh5V"
      },
      "source": [
        "### Data Preprocessing\n",
        "\n",
        "- Add special tokens to separate sentences and do classification\n",
        "- Pass sequences of constant length (introduce padding)\n",
        "- Create array of 0s (pad token) and 1s (real token) called *attention mask*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbzUI_IWgG3m"
      },
      "source": [
        "# Use this sample text from training set to understand the tokenization process:\n",
        "\n",
        "sample_text = df_train['Tweet'][0]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdR6mbwwdEea",
        "outputId": "b6471c33-798e-4423-bf59-1e55aeb33759"
      },
      "source": [
        "encoding = tokenizer.encode_plus(\n",
        "  sample_text,\n",
        "  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
        "  return_token_type_ids=False,\n",
        "  pad_to_max_length=True, # Add '[PAD]'\n",
        "  return_attention_mask=True,\n",
        "  return_tensors='pt', # Return PyTorch tensors\n",
        ")\n",
        "\n",
        "encoding.keys()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'attention_mask'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ul1y8gvhJ_I",
        "outputId": "470dfd34-1022-4c4f-85ee-2e6e3522fb60"
      },
      "source": [
        "# Inverse the tokenization to have a look at the special tokens:\n",
        "\n",
        "tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " '@',\n",
              " 'user',\n",
              " 'bono',\n",
              " '.',\n",
              " '.',\n",
              " '.',\n",
              " 'who',\n",
              " 'cares',\n",
              " '.',\n",
              " 'soon',\n",
              " 'people',\n",
              " 'will',\n",
              " 'understand',\n",
              " 'that',\n",
              " 'they',\n",
              " 'gain',\n",
              " 'nothing',\n",
              " 'from',\n",
              " 'following',\n",
              " 'a',\n",
              " 'ph',\n",
              " '##ony',\n",
              " 'celebrity',\n",
              " '.',\n",
              " 'become',\n",
              " 'a',\n",
              " 'leader',\n",
              " 'of',\n",
              " 'your',\n",
              " 'people',\n",
              " 'instead',\n",
              " 'or',\n",
              " 'help',\n",
              " 'and',\n",
              " 'support',\n",
              " 'your',\n",
              " 'fellow',\n",
              " 'country',\n",
              " '##men',\n",
              " '.',\n",
              " '[SEP]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ChJbDlbdBhK",
        "outputId": "b4a60092-62b4-4873-d6ca-8ad30d4d3efd"
      },
      "source": [
        "print(len(encoding['input_ids'][0]))\n",
        "encoding['input_ids'][0]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "42\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  101,  1030,  5310, 23648,  1012,  1012,  1012,  2040, 14977,  1012,\n",
              "         2574,  2111,  2097,  3305,  2008,  2027,  5114,  2498,  2013,  2206,\n",
              "         1037,  6887, 16585,  8958,  1012,  2468,  1037,  3003,  1997,  2115,\n",
              "         2111,  2612,  2030,  2393,  1998,  2490,  2115,  3507,  2406,  3549,\n",
              "         1012,   102])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZ8_oQrKdCqD",
        "outputId": "00705a05-f5b8-4287-fd16-74a19b124591"
      },
      "source": [
        "print(len(encoding['attention_mask'][0]))\n",
        "encoding['attention_mask']"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "42\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFXGNn_6lhYH"
      },
      "source": [
        "### Choosing Sequence Length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUXs4-AZEo-_"
      },
      "source": [
        "# Store the token length of each tweet:\n",
        "\n",
        "token_lens = []\n",
        "\n",
        "for tweet in df_train['Tweet']:\n",
        "  tokens = tokenizer.encode(tweet)\n",
        "  token_lens.append(len(tokens))"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "NwtwX0wJ2EA3",
        "outputId": "72523d8e-e3f2-4728-d20d-b19a6687f617"
      },
      "source": [
        "# Plot the distribution:\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.distplot(token_lens)\n",
        "plt.xlim([0, 200]);\n",
        "plt.xlabel('Token count');"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDru6wt3aChI"
      },
      "source": [
        "# Most of the tweet seem to contain less than 60 tokens, but we'll be on the safe side and choose a maximum length of 64.\n",
        "\n",
        "MAX_LEN = 128"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAUCWbbDE1-r"
      },
      "source": [
        "# Create a PyTorch dataset:\n",
        "\n",
        "class TweetDataset(Dataset):\n",
        "\n",
        "  def __init__(self, tweets, targets, tokenizer, max_len):\n",
        "    self.tweets = tweets\n",
        "    self.targets = targets\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.tweets)\n",
        "  \n",
        "  def __getitem__(self, item):\n",
        "    tweet = str(self.tweets[item])\n",
        "    target = self.targets[item]\n",
        "\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "      tweet,\n",
        "      add_special_tokens=True,\n",
        "      max_length=self.max_len,\n",
        "      return_token_type_ids=False,\n",
        "      pad_to_max_length=True,\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt',\n",
        "    )\n",
        "\n",
        "    return {\n",
        "      'tweet_text': tweet,\n",
        "      'input_ids': encoding['input_ids'].flatten(),\n",
        "      'attention_mask': encoding['attention_mask'].flatten(),\n",
        "      'targets': torch.tensor(target, dtype=torch.long)\n",
        "    }"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQzr9PifILOa"
      },
      "source": [
        "# Create data loader function:\n",
        "\n",
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "  ds = TweetDataset(\n",
        "    tweets=df['Tweet'].to_numpy(),\n",
        "    targets=df['Label'].to_numpy(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=max_len\n",
        "  )\n",
        "\n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=4\n",
        "  )"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kmJTNWRIaVV",
        "outputId": "d37e1908-e1d3-45ca-9196-a48b4289a1c9"
      },
      "source": [
        "# Choose a proper batch size and apply the data loader function to training data, validation data and test data:\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKFgVNSRIeNx",
        "outputId": "29b3dd53-8850-4766-a9d8-d3eee9b37dc5"
      },
      "source": [
        "# Have a look at an example batch from our training data loader:\n",
        "\n",
        "data = next(iter(train_data_loader))\n",
        "data.keys()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['tweet_text', 'input_ids', 'attention_mask', 'targets'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atulf3ZEIkae",
        "outputId": "9d5a1697-5ed2-4499-b777-1d90c0364d01"
      },
      "source": [
        "print(data['input_ids'].shape)\n",
        "print(data['attention_mask'].shape)\n",
        "print(data['targets'].shape)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([16, 128])\n",
            "torch.Size([16, 128])\n",
            "torch.Size([16])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3p8swfQmhPw"
      },
      "source": [
        "### Sentiment Classification with ERNIE 2.0 and Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2j2-IfTIqkV"
      },
      "source": [
        "# The last_hidden_state is a sequence of hidden states of the last layer of the model. Obtaining the pooled_output is done by applying the ERNIE Pooler on last_hidden_state:\n",
        "\n",
        "last_hidden_state, pooled_output = model(\n",
        "  input_ids=encoding['input_ids'], \n",
        "  attention_mask=encoding['attention_mask']\n",
        ")"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JC1V_bkIyB_",
        "outputId": "840452df-df2f-4f6a-a6c3-07d0a2c8c316"
      },
      "source": [
        "last_hidden_state.shape"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 42, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoibH_nUJWRD",
        "outputId": "350d175b-1704-46ae-c531-7cc361a1fd00"
      },
      "source": [
        "model.config.hidden_size"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mAOlPzZJXNz",
        "outputId": "4b9d517e-bfa3-42cd-e7cb-ec4fb3cd93cf"
      },
      "source": [
        "pooled_output.shape"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_jVm3mCJXxC"
      },
      "source": [
        "# We use all of the previous knowledge to create a sentiment classifier that uses ERNIE 2.0 model:\n",
        "\n",
        "class SentimentClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self, n_classes):\n",
        "    super(SentimentClassifier, self).__init__()\n",
        "    self.ernie = AutoModel.from_pretrained(\"nghuyong/ernie-2.0-en\", return_dict=False)\n",
        "    self.drop = nn.Dropout(p=0.3)\n",
        "    self.out = nn.Linear(self.ernie.config.hidden_size, n_classes)\n",
        "  \n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    _, pooled_output = self.ernie(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "    output = self.drop(pooled_output)\n",
        "    return self.out(output)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ficDwxuCLBde"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6-nfyyfKq_9"
      },
      "source": [
        "class_names = ['not-offensive', 'offensive']"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7LShQXoJfpj"
      },
      "source": [
        "# Create an instance and move it to the Colab GPU:\n",
        "\n",
        "model = SentimentClassifier(len(class_names))\n",
        "model = model.to(device)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPQNWvlEK4ir",
        "outputId": "aa435b35-ca4a-499e-feec-0c3eb649bfa8"
      },
      "source": [
        "# Move the example batch of our training data to the Colab GPU:\n",
        "\n",
        "input_ids = data['input_ids'].to(device)\n",
        "attention_mask = data['attention_mask'].to(device)\n",
        "\n",
        "print(input_ids.shape) # batch size x seq length\n",
        "print(attention_mask.shape) # batch size x seq length"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([16, 128])\n",
            "torch.Size([16, 128])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFtM_A19LIPJ",
        "outputId": "f0f6a8b5-8180-4069-88a5-965ded81d428"
      },
      "source": [
        "model(input_ids, attention_mask)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1647,  0.3215],\n",
              "        [ 0.4343,  0.3459],\n",
              "        [ 0.0238, -0.4144],\n",
              "        [-0.1698, -0.1869],\n",
              "        [-0.2536, -0.2827],\n",
              "        [-0.1187, -0.1317],\n",
              "        [-0.6639,  0.3726],\n",
              "        [-0.1381,  0.2858],\n",
              "        [-0.1741, -0.2641],\n",
              "        [ 0.5825, -0.3806],\n",
              "        [ 0.2252,  0.0172],\n",
              "        [-0.2510,  0.1399],\n",
              "        [ 0.3558, -0.1499],\n",
              "        [-0.0619, -0.0396],\n",
              "        [-0.5224,  0.0171],\n",
              "        [ 0.1511, -0.1203]], device='cuda:0', grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIastka-nXTV"
      },
      "source": [
        "### Model training\n",
        " \n",
        "Although our datasets are small, we set the hyperparameters (batch size, learning rate and number of epoch) at reasonable values in order to avoid over-fitting and to obtain high prediction accuracy. We use the AdamW optimizer provided by Hugging Face that could correct weight decay. We also use a linear scheduler with no warmup steps.\n",
        "\n",
        "- Optimizer: AdamW\n",
        "- Batch size: 16\n",
        "- Learning rate : 2e-5\n",
        "- Number of epochs: 10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOKZkx1SMLbj"
      },
      "source": [
        "# Set up epochs, optimizer and scheduler:\n",
        "\n",
        "EPOCHS = 10\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyXbuHUfTmSO"
      },
      "source": [
        "# A helper function for training our model for one epoch:\n",
        "\n",
        "def train_epoch(\n",
        "  model, \n",
        "  data_loader, \n",
        "  loss_fn, \n",
        "  optimizer, \n",
        "  device, \n",
        "  scheduler, \n",
        "  n_examples\n",
        "):\n",
        "  model = model.train()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  \n",
        "  for d in data_loader:\n",
        "    input_ids = d[\"input_ids\"].to(device)\n",
        "    attention_mask = d[\"attention_mask\"].to(device)\n",
        "    targets = d[\"targets\"].to(device)\n",
        "\n",
        "    outputs = model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    loss = loss_fn(outputs, targets)\n",
        "\n",
        "    correct_predictions += torch.sum(preds == targets)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNgWx_kFTNaF"
      },
      "source": [
        "# Another helper function that helps us evaluate the model on a given data loader:\n",
        "\n",
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      loss = loss_fn(outputs, targets)\n",
        "\n",
        "      correct_predictions += torch.sum(preds == targets)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mF5T6H4T4qN",
        "outputId": "27de2837-0bd4-4aaa-dd4e-ea313ecc8499"
      },
      "source": [
        "# Using those two helper functions, we can write our training loop. We'll also store the training history:\n",
        "\n",
        "%%time\n",
        "\n",
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "  print('-' * 10)\n",
        "\n",
        "  train_acc, train_loss = train_epoch(\n",
        "    model,\n",
        "    train_data_loader,    \n",
        "    loss_fn, \n",
        "    optimizer, \n",
        "    device, \n",
        "    scheduler, \n",
        "    len(df_train)\n",
        "  )\n",
        "\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "  val_acc, val_loss = eval_model(\n",
        "    model,\n",
        "    val_data_loader,\n",
        "    loss_fn, \n",
        "    device, \n",
        "    len(df_val)\n",
        "  )\n",
        "\n",
        "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "  print()\n",
        "\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "\n",
        "  if val_acc > best_accuracy:\n",
        "    torch.save(model.state_dict(), 'best_model_state.bin')\n",
        "    best_accuracy = val_acc"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.5032237859780356 accuracy 0.772322927156764\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.44188795391335545 accuracy 0.7885196374622356\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.4221098029873515 accuracy 0.8290533736153072\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.4863216850053833 accuracy 0.7847432024169184\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.3702244436480855 accuracy 0.8627895266868076\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.5238068163035864 accuracy 0.7900302114803626\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.3336987562177565 accuracy 0.8908190668009399\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.6151942997632256 accuracy 0.7885196374622356\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.31870653548796707 accuracy 0.9033232628398792\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.6062508690429022 accuracy 0.7960725075528701\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.29646864418915453 accuracy 0.9131419939577039\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.651895370603685 accuracy 0.7892749244712991\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.27130534912165777 accuracy 0.9241356159785162\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.6804373671401696 accuracy 0.7922960725075529\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.24958852243753488 accuracy 0.9328633769721383\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.7007317466728659 accuracy 0.790785498489426\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2399939482844116 accuracy 0.9376468613628735\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.7266255717006433 accuracy 0.7885196374622356\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.23024503500169555 accuracy 0.9405840886203424\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.7157046080578163 accuracy 0.7968277945619335\n",
            "\n",
            "CPU times: user 27min 22s, sys: 21min 24s, total: 48min 47s\n",
            "Wall time: 49min 1s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "YqjE6UBaUD0g",
        "outputId": "a61eec7d-1be5-4f79-8dac-77c107cc5321"
      },
      "source": [
        "# Plot the training accuracy and validation accuracy of each epoch:\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(history['train_acc'], label='train accuracy')\n",
        "plt.plot(history['val_acc'], label='validation accuracy')\n",
        "\n",
        "plt.title('Training history (Offensive Language Identification)')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIMAdCz4pfas"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "Calculating the accuracy on our test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZS3w26Ooim-_",
        "outputId": "4222434d-537d-4ad4-ce04-957ecb3ea18f"
      },
      "source": [
        "# Calculating the accuracy on the test set:\n",
        "\n",
        "test_acc, _ = eval_model(\n",
        "  model,\n",
        "  test_data_loader,\n",
        "  loss_fn,\n",
        "  device,\n",
        "  len(df_test)\n",
        ")\n",
        "test_acc.item()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8348837209302326"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "775k6ZTalu2-"
      },
      "source": [
        "# Define a helper function to get the predictions from our model:\n",
        "\n",
        "def get_predictions(model, data_loader):\n",
        "  model = model.eval()\n",
        "  tweet_texts = []\n",
        "  predictions = []\n",
        "  prediction_probs = []\n",
        "  real_values = []\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      texts = d[\"tweet_text\"]\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "      tweet_texts.extend(texts)\n",
        "      predictions.extend(preds)\n",
        "      prediction_probs.extend(outputs)\n",
        "      real_values.extend(targets)\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
        "  real_values = torch.stack(real_values).cpu()\n",
        "  return tweet_texts, predictions, prediction_probs, real_values"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "In4t-Tc3mF4O",
        "outputId": "41d62328-80d5-4073-989a-e5ea940c5ca3"
      },
      "source": [
        "# Storing the text of tweets and predicted probabilities:\n",
        "\n",
        "y_tweet_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
        "  model,\n",
        "  test_data_loader\n",
        ")"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAWZwMjjmIbM",
        "outputId": "77b4ca14-e1bc-4eb0-a200-fdb7a7948b29"
      },
      "source": [
        "# Print the classification report:\n",
        "\n",
        "print(classification_report(y_test, y_pred, target_names=class_names))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "               precision    recall  f1-score   support\n",
            "\n",
            "not-offensive       0.86      0.92      0.89       620\n",
            "    offensive       0.74      0.62      0.68       240\n",
            "\n",
            "     accuracy                           0.83       860\n",
            "    macro avg       0.80      0.77      0.78       860\n",
            " weighted avg       0.83      0.83      0.83       860\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "_RLU8dE9mUrI",
        "outputId": "a0794846-8f4a-4d8d-fac8-daf2e0cfe3df"
      },
      "source": [
        "# Plot the confusion matrix:\n",
        "\n",
        "def show_confusion_matrix(confusion_matrix):\n",
        "  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
        "  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
        "  plt.ylabel('True sentiment')\n",
        "  plt.xlabel('Predicted sentiment');\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
        "show_confusion_matrix(df_cm)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hQTQaecmYuA"
      },
      "source": [
        "# Check an example tweet from the test set:\n",
        "\n",
        "idx = 2\n",
        "tweet_text = y_tweet_texts[idx]\n",
        "true_sentiment = y_test[idx]\n",
        "pred_df = pd.DataFrame({\n",
        "  'class_names': class_names,\n",
        "  'values': y_pred_probs[idx]\n",
        "})"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXS_1enbmfWX",
        "outputId": "ffea8797-def1-4295-952c-6141b7188a00"
      },
      "source": [
        "print(\"\\n\".join(wrap(tweet_text)))\n",
        "print()\n",
        "print(f'True sentiment: {class_names[true_sentiment]}')"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "...if you want more shootings and more death, then listen to the ACLU,\n",
            "Black Lives Matter, or Antifa. If you want public safety, then listen\n",
            "to the police professionals who have been studying this for 35\n",
            "years.\"\" -AG Jeff Sessions\n",
            "\n",
            "True sentiment: not-offensive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "1Bk3_bfpoKlQ",
        "outputId": "2aaa79f0-f9c3-4708-e0c6-5508095a4e70"
      },
      "source": [
        "# Look at the confidence of each sentiment of our model:\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x='values', y='class_names', data=pred_df, orient='h')\n",
        "plt.ylabel('sentiment')\n",
        "plt.xlabel('probability')\n",
        "plt.xlim([0, 1]);"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyGgOJhRovY-"
      },
      "source": [
        ""
      ],
      "execution_count": 57,
      "outputs": []
    }
  ]
}